{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import mnist_inference\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100 \n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 30001\n",
    "MOVING_AVERAGE_DECAY = 0.99 \n",
    "MODEL_SAVE_PATH = \"TianChi_Model/\"\n",
    "MODEL_NAME = \"tianchi_model\"\n",
    "TENSORBOARD_LOG = 'tensor_board'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(trainX, trainY):\n",
    "    dataSize = len(trainY)\n",
    "    \n",
    "    # 定义输入输出placeholder。\n",
    "    x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name='y-input')\n",
    "\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    y = mnist_inference.inference(x, regularizer)\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # 定义损失函数、学习率、滑动平均操作以及训练过程。\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "#     cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "#     cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    beginLoss = tf.reduce_mean(tf.abs(tf.subtract(y, y_)))\n",
    "#     loss = beginLoss + tf.add_n(tf.get_collection('losses'))\n",
    "    loss = beginLoss\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        dataSize / BATCH_SIZE, LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "        \n",
    "    # 初始化TensorFlow持久化类。\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        head = 0\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            tail = head+BATCH_SIZE\n",
    "            if tail > dataSize:\n",
    "                xs = np.concatenate((trainX[head: BATCH_SIZE], trainX[0: tail-BATCH_SIZE]))\n",
    "                ys = np.concatenate((trainY[head: BATCH_SIZE], trainY[0: tail-BATCH_SIZE]))\n",
    "                head = tail - BATCH_SIZE\n",
    "            else:\n",
    "                xs, ys = trainX[head: head+BATCH_SIZE-1], trainY[head: head+BATCH_SIZE-1]\n",
    "                head = tail\n",
    "            \n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n",
    "            if i % 1000 == 0:\n",
    "                print(\"After\", step,\" training step(s), loss on training batch is \", loss_value)\n",
    "                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:\\\\workspace\\\\Tensorflow\\\\tensorflow-tutorial-master\\\\Deep_Learning_with_TensorFlow\\\\1.0.0\\\\TianChi\\\\final.csv')\n",
    "# random shift the df\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "normalizeColumns = ['compartment','TR','displacement','price_level','power','level_id',\n",
    "                    'cylinder_number','engine_torque','car_length','car_height','car_width','total_quality','equipment_quality',\n",
    "                    'rated_passenger','wheelbase','front_track','rear_track']\n",
    "leftDf = df.drop(normalizeColumns, axis =1 ).drop(['sale_quantity'], axis = 1)\n",
    "\n",
    "normalizeDf = df[normalizeColumns]\n",
    "normalizeDf = (normalizeDf-normalizeDf.min())/(normalizeDf.max()-normalizeDf.min())\n",
    "inputDf = pd.concat([leftDf, normalizeDf], axis = 1)\n",
    "inputX = inputDf.values\n",
    "resultArray = df['sale_quantity'].values\n",
    "inputY = resultArray.reshape((len(resultArray),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1  training step(s), loss on training batch is  113.558\n",
      "After 1001  training step(s), loss on training batch is  83.1199\n",
      "After 2001  training step(s), loss on training batch is  73.484\n",
      "After 3001  training step(s), loss on training batch is  66.4081\n",
      "After 4001  training step(s), loss on training batch is  64.5243\n",
      "After 5001  training step(s), loss on training batch is  64.1737\n",
      "After 6001  training step(s), loss on training batch is  58.0867\n",
      "After 7001  training step(s), loss on training batch is  56.8634\n",
      "After 8001  training step(s), loss on training batch is  58.7797\n",
      "After 9001  training step(s), loss on training batch is  56.1718\n",
      "After 10001  training step(s), loss on training batch is  53.6385\n",
      "After 11001  training step(s), loss on training batch is  48.4368\n",
      "After 12001  training step(s), loss on training batch is  49.2943\n",
      "After 13001  training step(s), loss on training batch is  45.7386\n",
      "After 14001  training step(s), loss on training batch is  55.9156\n",
      "After 15001  training step(s), loss on training batch is  47.8815\n",
      "After 16001  training step(s), loss on training batch is  43.6912\n",
      "After 17001  training step(s), loss on training batch is  44.5964\n",
      "After 18001  training step(s), loss on training batch is  44.5181\n",
      "After 19001  training step(s), loss on training batch is  45.5099\n",
      "After 20001  training step(s), loss on training batch is  43.9195\n",
      "After 21001  training step(s), loss on training batch is  40.4052\n",
      "After 22001  training step(s), loss on training batch is  39.7552\n",
      "After 23001  training step(s), loss on training batch is  39.8508\n"
     ]
    }
   ],
   "source": [
    "train(inputX, inputY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
