{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import mnist_inference\n",
    "import Layer2Model\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100 \n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 200001\n",
    "MOVING_AVERAGE_DECAY = 0.99 \n",
    "MODEL_SAVE_PATH = \"TianChi_Model/\"\n",
    "MODEL_NAME = \"tianchi_model\"\n",
    "two_layers_model_fileName = '2HiddenLayer'\n",
    "TENSORBOARD_LOG = 'tensor_board'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def startTrain(trainX, trainY, model_path, model_name, model):\n",
    "    dataSize = len(trainY)\n",
    "    \n",
    "    with tf.device('/device:GPU:0'):\n",
    "    # 定义输入输出placeholder。\n",
    "        x = tf.placeholder(tf.float32, [None, model.INPUT_NODE], name='x-input')\n",
    "        y_ = tf.placeholder(tf.float32, [None, model.OUTPUT_NODE], name='y-input')\n",
    "\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "        y = model.inference(x, regularizer)\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # 定义损失函数、学习率、滑动平均操作以及训练过程。\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "#     cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "#     cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "        beginLoss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(y, y_))))\n",
    "#     loss = beginLoss + tf.add_n(tf.get_collection('losses'))\n",
    "        loss = beginLoss\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            LEARNING_RATE_BASE,\n",
    "            global_step,\n",
    "            dataSize / BATCH_SIZE, LEARNING_RATE_DECAY,\n",
    "            staircase=True)\n",
    "        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "        with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "            train_op = tf.no_op(name='train')\n",
    "        \n",
    "    # 初始化TensorFlow持久化类。\n",
    "    saver = tf.train.Saver()\n",
    "    config = tf.ConfigProto(allow_soft_placement = True, log_device_placement=True)\n",
    "    with tf.Session(config = config) as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        head = 0\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            tail = head+BATCH_SIZE\n",
    "            if tail > dataSize:\n",
    "                xs = np.concatenate((trainX[head: BATCH_SIZE], trainX[0: tail-BATCH_SIZE]))\n",
    "                ys = np.concatenate((trainY[head: BATCH_SIZE], trainY[0: tail-BATCH_SIZE]))\n",
    "                head = tail - BATCH_SIZE\n",
    "            else:\n",
    "                xs, ys = trainX[head: head+BATCH_SIZE-1], trainY[head: head+BATCH_SIZE-1]\n",
    "                head = tail\n",
    "            \n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n",
    "            if i % 1000 == 0:\n",
    "                print(\"After\", step,\" training step(s), loss on training batch is \", loss_value)\n",
    "            if i % 10000 == 0:\n",
    "                saver.save(sess, os.path.join(model_path, model_name), global_step=global_step)\n",
    "                testLoss = sess.run([loss], feed_dict={x: testX, y_: testY})\n",
    "                print(\"After\", step,\" training step(s), loss on test set is \", testLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# handle data, produce train and test input\n",
    "\n",
    "df = pd.read_csv('final.csv')\n",
    "# random shift the df\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "normalizeColumns = ['compartment','TR','displacement','price_level','power','level_id',\n",
    "                    'cylinder_number','engine_torque','car_length','car_height','car_width','total_quality','equipment_quality',\n",
    "                    'rated_passenger','wheelbase','front_track','rear_track']\n",
    "leftDf = df.drop(normalizeColumns, axis =1 ).drop(['sale_quantity'], axis = 1)\n",
    "\n",
    "normalizeDf = df[normalizeColumns]\n",
    "normalizeDf = (normalizeDf-normalizeDf.min())/(normalizeDf.max()-normalizeDf.min())\n",
    "inputDf = pd.concat([leftDf, normalizeDf], axis = 1)\n",
    "inputX = inputDf.values\n",
    "resultArray = df['sale_quantity'].values\n",
    "inputY = resultArray.reshape((len(resultArray),1))\n",
    "trainX = inputX[0:18000]\n",
    "trainY = inputY[0:18000]\n",
    "testX = inputX[18000:]\n",
    "testY = inputY[18000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def continueTrain(trainX, trainY, sess, continue_steps):\n",
    "    head = 0\n",
    "    dataSize = len(trainY)\n",
    "    for i in range(continue_steps):\n",
    "        tail = head+BATCH_SIZE\n",
    "        if tail > dataSize:\n",
    "            xs = np.concatenate((trainX[head: BATCH_SIZE], trainX[0: tail-BATCH_SIZE]))\n",
    "            ys = np.concatenate((trainY[head: BATCH_SIZE], trainY[0: tail-BATCH_SIZE]))\n",
    "            head = tail - BATCH_SIZE\n",
    "        else:\n",
    "            xs, ys = trainX[head: head+BATCH_SIZE-1], trainY[head: head+BATCH_SIZE-1]\n",
    "            head = tail\n",
    "            \n",
    "        _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n",
    "        if i % 1000 == 0:\n",
    "            print(\"After\", step,\" training step(s), loss on training batch is \", loss_value)\n",
    "        if i % 10000 == 0:\n",
    "            saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n",
    "            testLoss = sess.run([loss], feed_dict={x: testX, y_: testY})\n",
    "            print(\"After\", step,\" training step(s), loss on test set is \", restoredSession.run([loss], feed_dict={x: testX, y_: testY}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1  training step(s), loss on training batch is  241.81886\n",
      "After 1  training step(s), loss on test set is  [254.51508]\n",
      "After 1001  training step(s), loss on training batch is  164.55571\n",
      "After 2001  training step(s), loss on training batch is  145.78275\n",
      "After 3001  training step(s), loss on training batch is  137.77367\n",
      "After 4001  training step(s), loss on training batch is  130.30962\n",
      "After 5001  training step(s), loss on training batch is  124.78074\n",
      "After 6001  training step(s), loss on training batch is  117.69403\n",
      "After 7001  training step(s), loss on training batch is  113.80787\n",
      "After 8001  training step(s), loss on training batch is  109.756874\n",
      "After 9001  training step(s), loss on training batch is  108.107475\n",
      "After 10001  training step(s), loss on training batch is  105.22526\n",
      "After 10001  training step(s), loss on test set is  [197.53035]\n",
      "After 11001  training step(s), loss on training batch is  99.80518\n",
      "After 12001  training step(s), loss on training batch is  101.03128\n",
      "After 13001  training step(s), loss on training batch is  95.208824\n",
      "After 14001  training step(s), loss on training batch is  94.4504\n",
      "After 15001  training step(s), loss on training batch is  93.62566\n",
      "After 16001  training step(s), loss on training batch is  92.14269\n",
      "After 17001  training step(s), loss on training batch is  90.315834\n",
      "After 18001  training step(s), loss on training batch is  89.56594\n",
      "After 19001  training step(s), loss on training batch is  87.59152\n",
      "After 20001  training step(s), loss on training batch is  86.02937\n",
      "After 20001  training step(s), loss on test set is  [209.01482]\n",
      "After 21001  training step(s), loss on training batch is  88.278915\n",
      "After 22001  training step(s), loss on training batch is  85.59609\n",
      "After 23001  training step(s), loss on training batch is  84.41596\n",
      "After 24001  training step(s), loss on training batch is  85.75585\n",
      "After 25001  training step(s), loss on training batch is  82.88751\n",
      "After 26001  training step(s), loss on training batch is  82.33841\n",
      "After 27001  training step(s), loss on training batch is  83.502754\n",
      "After 28001  training step(s), loss on training batch is  82.55737\n",
      "After 29001  training step(s), loss on training batch is  80.87361\n",
      "After 30001  training step(s), loss on training batch is  80.99586\n",
      "After 30001  training step(s), loss on test set is  [212.5659]\n",
      "After 31001  training step(s), loss on training batch is  80.07655\n",
      "After 32001  training step(s), loss on training batch is  79.85973\n",
      "After 33001  training step(s), loss on training batch is  79.90317\n",
      "After 34001  training step(s), loss on training batch is  79.56091\n",
      "After 35001  training step(s), loss on training batch is  78.817345\n",
      "After 36001  training step(s), loss on training batch is  78.783035\n",
      "After 37001  training step(s), loss on training batch is  78.318985\n",
      "After 38001  training step(s), loss on training batch is  78.18382\n",
      "After 39001  training step(s), loss on training batch is  77.89042\n",
      "After 40001  training step(s), loss on training batch is  77.694954\n",
      "After 40001  training step(s), loss on test set is  [211.70761]\n",
      "After 41001  training step(s), loss on training batch is  77.55086\n",
      "After 42001  training step(s), loss on training batch is  77.341515\n",
      "After 43001  training step(s), loss on training batch is  77.18487\n",
      "After 44001  training step(s), loss on training batch is  77.027725\n",
      "After 45001  training step(s), loss on training batch is  76.8815\n",
      "After 46001  training step(s), loss on training batch is  76.74835\n",
      "After 47001  training step(s), loss on training batch is  76.60879\n",
      "After 48001  training step(s), loss on training batch is  76.47086\n",
      "After 49001  training step(s), loss on training batch is  76.344185\n",
      "After 50001  training step(s), loss on training batch is  76.221306\n",
      "After 50001  training step(s), loss on test set is  [211.74814]\n",
      "After 51001  training step(s), loss on training batch is  76.10969\n",
      "After 52001  training step(s), loss on training batch is  75.99494\n",
      "After 53001  training step(s), loss on training batch is  75.89426\n",
      "After 54001  training step(s), loss on training batch is  75.79856\n",
      "After 55001  training step(s), loss on training batch is  75.707146\n",
      "After 56001  training step(s), loss on training batch is  75.623116\n",
      "After 57001  training step(s), loss on training batch is  75.54722\n",
      "After 58001  training step(s), loss on training batch is  75.47133\n",
      "After 59001  training step(s), loss on training batch is  75.36989\n",
      "After 60001  training step(s), loss on training batch is  75.299286\n",
      "After 60001  training step(s), loss on test set is  [211.47343]\n",
      "After 61001  training step(s), loss on training batch is  75.235664\n",
      "After 62001  training step(s), loss on training batch is  75.17473\n",
      "After 63001  training step(s), loss on training batch is  75.10998\n",
      "After 64001  training step(s), loss on training batch is  75.053116\n",
      "After 65001  training step(s), loss on training batch is  75.00191\n",
      "After 66001  training step(s), loss on training batch is  74.95213\n",
      "After 67001  training step(s), loss on training batch is  74.90631\n",
      "After 68001  training step(s), loss on training batch is  74.863525\n",
      "After 69001  training step(s), loss on training batch is  74.82381\n",
      "After 70001  training step(s), loss on training batch is  74.77701\n",
      "After 70001  training step(s), loss on test set is  [211.40584]\n",
      "After 71001  training step(s), loss on training batch is  74.74047\n",
      "After 72001  training step(s), loss on training batch is  74.70584\n",
      "After 73001  training step(s), loss on training batch is  74.67124\n",
      "After 74001  training step(s), loss on training batch is  74.63582\n",
      "After 75001  training step(s), loss on training batch is  74.604454\n",
      "After 76001  training step(s), loss on training batch is  74.57125\n",
      "After 77001  training step(s), loss on training batch is  74.543465\n",
      "After 78001  training step(s), loss on training batch is  74.516495\n",
      "After 79001  training step(s), loss on training batch is  74.49238\n",
      "After 80001  training step(s), loss on training batch is  74.46932\n",
      "After 80001  training step(s), loss on test set is  [211.37952]\n",
      "After 81001  training step(s), loss on training batch is  74.44749\n",
      "After 82001  training step(s), loss on training batch is  74.42674\n",
      "After 83001  training step(s), loss on training batch is  74.40694\n",
      "After 84001  training step(s), loss on training batch is  74.38858\n",
      "After 85001  training step(s), loss on training batch is  74.37054\n",
      "After 86001  training step(s), loss on training batch is  74.35022\n",
      "After 87001  training step(s), loss on training batch is  74.33469\n",
      "After 88001  training step(s), loss on training batch is  74.31998\n",
      "After 89001  training step(s), loss on training batch is  74.30613\n",
      "After 90001  training step(s), loss on training batch is  74.29288\n",
      "After 90001  training step(s), loss on test set is  [211.32808]\n",
      "After 91001  training step(s), loss on training batch is  74.280266\n",
      "After 92001  training step(s), loss on training batch is  74.268265\n",
      "After 93001  training step(s), loss on training batch is  74.257034\n",
      "After 94001  training step(s), loss on training batch is  74.246506\n",
      "After 95001  training step(s), loss on training batch is  74.23608\n",
      "After 96001  training step(s), loss on training batch is  74.22649\n",
      "After 97001  training step(s), loss on training batch is  74.21759\n",
      "After 98001  training step(s), loss on training batch is  74.2092\n",
      "After 99001  training step(s), loss on training batch is  74.20118\n",
      "After 100001  training step(s), loss on training batch is  74.193825\n",
      "After 100001  training step(s), loss on test set is  [211.30267]\n",
      "After 101001  training step(s), loss on training batch is  74.186676\n",
      "After 102001  training step(s), loss on training batch is  74.18002\n",
      "After 103001  training step(s), loss on training batch is  74.173775\n",
      "After 104001  training step(s), loss on training batch is  74.16766\n",
      "After 105001  training step(s), loss on training batch is  74.16197\n",
      "After 106001  training step(s), loss on training batch is  74.15646\n",
      "After 107001  training step(s), loss on training batch is  74.15148\n",
      "After 108001  training step(s), loss on training batch is  74.146645\n",
      "After 109001  training step(s), loss on training batch is  74.14207\n",
      "After 110001  training step(s), loss on training batch is  74.137794\n",
      "After 110001  training step(s), loss on test set is  [211.29416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 111001  training step(s), loss on training batch is  74.13375\n",
      "After 112001  training step(s), loss on training batch is  74.12999\n",
      "After 113001  training step(s), loss on training batch is  74.12647\n",
      "After 114001  training step(s), loss on training batch is  74.12302\n",
      "After 115001  training step(s), loss on training batch is  74.11981\n",
      "After 116001  training step(s), loss on training batch is  74.11674\n",
      "After 117001  training step(s), loss on training batch is  74.11384\n",
      "After 118001  training step(s), loss on training batch is  74.11122\n",
      "After 119001  training step(s), loss on training batch is  74.10871\n",
      "After 120001  training step(s), loss on training batch is  74.106255\n",
      "After 120001  training step(s), loss on test set is  [211.29446]\n",
      "After 121001  training step(s), loss on training batch is  74.104004\n",
      "After 122001  training step(s), loss on training batch is  74.1018\n",
      "After 123001  training step(s), loss on training batch is  74.09965\n",
      "After 124001  training step(s), loss on training batch is  74.09768\n",
      "After 125001  training step(s), loss on training batch is  74.09578\n",
      "After 126001  training step(s), loss on training batch is  74.09404\n",
      "After 127001  training step(s), loss on training batch is  74.092445\n",
      "After 128001  training step(s), loss on training batch is  74.09094\n",
      "After 129001  training step(s), loss on training batch is  74.08954\n",
      "After 130001  training step(s), loss on training batch is  74.08823\n",
      "After 130001  training step(s), loss on test set is  [211.29204]\n",
      "After 131001  training step(s), loss on training batch is  74.08698\n",
      "After 132001  training step(s), loss on training batch is  74.08588\n",
      "After 133001  training step(s), loss on training batch is  74.08478\n",
      "After 134001  training step(s), loss on training batch is  74.083755\n",
      "After 135001  training step(s), loss on training batch is  74.0828\n",
      "After 136001  training step(s), loss on training batch is  74.08186\n",
      "After 137001  training step(s), loss on training batch is  74.08099\n",
      "After 138001  training step(s), loss on training batch is  74.08017\n",
      "After 139001  training step(s), loss on training batch is  74.0794\n",
      "After 140001  training step(s), loss on training batch is  74.078636\n",
      "After 140001  training step(s), loss on test set is  [211.2827]\n",
      "After 141001  training step(s), loss on training batch is  74.07794\n",
      "After 142001  training step(s), loss on training batch is  74.07728\n",
      "After 143001  training step(s), loss on training batch is  74.07664\n",
      "After 144001  training step(s), loss on training batch is  74.07605\n",
      "After 145001  training step(s), loss on training batch is  74.0755\n",
      "After 146001  training step(s), loss on training batch is  74.07504\n",
      "After 147001  training step(s), loss on training batch is  74.07466\n",
      "After 148001  training step(s), loss on training batch is  74.074295\n",
      "After 149001  training step(s), loss on training batch is  74.073975\n",
      "After 150001  training step(s), loss on training batch is  74.0737\n",
      "After 150001  training step(s), loss on test set is  [211.27858]\n",
      "After 151001  training step(s), loss on training batch is  74.07344\n",
      "After 152001  training step(s), loss on training batch is  74.07323\n",
      "After 153001  training step(s), loss on training batch is  74.07304\n",
      "After 154001  training step(s), loss on training batch is  74.07288\n",
      "After 155001  training step(s), loss on training batch is  74.07275\n",
      "After 156001  training step(s), loss on training batch is  74.072624\n",
      "After 157001  training step(s), loss on training batch is  74.07252\n",
      "After 158001  training step(s), loss on training batch is  74.07242\n",
      "After 159001  training step(s), loss on training batch is  74.072334\n",
      "After 160001  training step(s), loss on training batch is  74.07224\n",
      "After 160001  training step(s), loss on test set is  [211.27666]\n",
      "After 161001  training step(s), loss on training batch is  74.07217\n",
      "After 162001  training step(s), loss on training batch is  74.072105\n",
      "After 163001  training step(s), loss on training batch is  74.07204\n",
      "After 164001  training step(s), loss on training batch is  74.07199\n",
      "After 165001  training step(s), loss on training batch is  74.07194\n",
      "After 166001  training step(s), loss on training batch is  74.0719\n",
      "After 167001  training step(s), loss on training batch is  74.07186\n",
      "After 168001  training step(s), loss on training batch is  74.07183\n",
      "After 169001  training step(s), loss on training batch is  74.0718\n",
      "After 170001  training step(s), loss on training batch is  74.07178\n",
      "After 170001  training step(s), loss on test set is  [211.27422]\n",
      "After 171001  training step(s), loss on training batch is  74.07176\n",
      "After 172001  training step(s), loss on training batch is  74.07174\n",
      "After 173001  training step(s), loss on training batch is  74.07172\n",
      "After 174001  training step(s), loss on training batch is  74.0717\n",
      "After 175001  training step(s), loss on training batch is  74.07169\n",
      "After 176001  training step(s), loss on training batch is  74.07168\n",
      "After 177001  training step(s), loss on training batch is  74.07166\n",
      "After 178001  training step(s), loss on training batch is  74.07166\n",
      "After 179001  training step(s), loss on training batch is  74.07165\n",
      "After 180001  training step(s), loss on training batch is  74.07164\n",
      "After 180001  training step(s), loss on test set is  [211.27339]\n",
      "After 181001  training step(s), loss on training batch is  74.07163\n",
      "After 182001  training step(s), loss on training batch is  74.071625\n",
      "After 183001  training step(s), loss on training batch is  74.07162\n",
      "After 184001  training step(s), loss on training batch is  74.07162\n",
      "After 185001  training step(s), loss on training batch is  74.07161\n",
      "After 186001  training step(s), loss on training batch is  74.0716\n",
      "After 187001  training step(s), loss on training batch is  74.0716\n",
      "After 188001  training step(s), loss on training batch is  74.071594\n",
      "After 189001  training step(s), loss on training batch is  74.071594\n",
      "After 190001  training step(s), loss on training batch is  74.071594\n",
      "After 190001  training step(s), loss on test set is  [211.27213]\n",
      "After 191001  training step(s), loss on training batch is  74.071594\n",
      "After 192001  training step(s), loss on training batch is  74.07159\n",
      "After 193001  training step(s), loss on training batch is  74.07159\n",
      "After 194001  training step(s), loss on training batch is  74.07158\n",
      "After 195001  training step(s), loss on training batch is  74.07158\n",
      "After 196001  training step(s), loss on training batch is  74.07158\n",
      "After 197001  training step(s), loss on training batch is  74.07157\n",
      "After 198001  training step(s), loss on training batch is  74.07158\n",
      "After 199001  training step(s), loss on training batch is  74.07157\n",
      "After 200001  training step(s), loss on training batch is  74.07157\n",
      "After 200001  training step(s), loss on test set is  [211.27202]\n"
     ]
    }
   ],
   "source": [
    "startTrain(trainX, trainY, MODEL_SAVE_PATH, MODEL_NAME, mnist_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 初始化TensorFlow持久化类。\n",
    "# saver = tf.train.Saver()\n",
    "# config = tf.ConfigProto(allow_soft_placement = True, log_device_placement=True)\n",
    "# sess = tf.Session(config = config)\n",
    "# with sess:\n",
    "#     tf.global_variables_initializer().run()\n",
    "        \n",
    "#     head = 0\n",
    "#     for i in range(TRAINING_STEPS):\n",
    "#         tail = head+BATCH_SIZE\n",
    "#         if tail > dataSize:\n",
    "#             xs = np.concatenate((trainX[head: BATCH_SIZE], trainX[0: tail-BATCH_SIZE]))\n",
    "#             ys = np.concatenate((trainY[head: BATCH_SIZE], trainY[0: tail-BATCH_SIZE]))\n",
    "#             head = tail - BATCH_SIZE\n",
    "#         else:\n",
    "#             xs, ys = trainX[head: head+BATCH_SIZE-1], trainY[head: head+BATCH_SIZE-1]\n",
    "#             head = tail\n",
    "            \n",
    "#         _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n",
    "#         if i % 1000 == 0:\n",
    "#             print(\"After\", step,\" training step(s), loss on training batch is \", loss_value)\n",
    "#         if i % 5000 == 0:\n",
    "#             saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = tf.ConfigProto(allow_soft_placement = True, log_device_placement=True)\n",
    "# restoredSession = tf.Session(config=config)\n",
    "# restoredSaver = tf.train.import_meta_graph(os.path.join(MODEL_SAVE_PATH, 'tianchi_model-30001.meta'))\n",
    "# restoredSaver.restore(restoredSession, os.path.join(MODEL_SAVE_PATH, 'tianchi_model-30001'))\n",
    "# # restoredSession.run([loss], feed_dict={x: testX, y_: testY})\n",
    "# continueTrain(trainX, trainY, restoredSession, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
